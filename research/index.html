---
layout: default
title: My Research
js: research.js
---
<section>
	<h4>My research examines the design and development of large-scale software solutions for disaster preparedness, response, and recovery.</h4>
  
	<p> With the wake of pervasive information and communication technology (ICT) in the hands of the public citizenry, the information interface between formal emergency management and the public is widening, enabling opportunities to build ICT for large-scale situational awareness of disaster-related information. Volunteer technology communities (VTCs), like Ushahidi and OpenStreetMap, mobilize volunteer technologists - developers, analysts, and designers from all over the world - to participate in humanitarian efforts that aid thousands of disaster victims.</p>

	<p>However, software in this domain, the way we build it, and the way we use it, are not ideal. Mobile and web apps need to be scalable or risk falling over from unexpected bugs; they need to be designed for the user or risk churn; but most of all, their actual use in the context of disaster preparedness, response, and recovery needs to be well understood, or it risks not being used at all. In a decentralized work environment, VTCs fall victim to global software engineering issues, like regional, temporal, and cultural differences and collaboration issues in round-the-clock development. The fragmentation of skillsets also makes it hard to coordinate tasks.</p>

	<p>My goal is twofold: first, to ethnographically examine the dynamics of VTC organizations, observing the toolsets that they use in order to get the job done; second, based on the collected data, to determine best practice engineering tools and techniques that increase productivity and efficiency, as well as to bridge real-world disaster scenarios with these insights to make arguments for situated use. New observations and insights can help realize best practices for how large-scale, decentralized software development in disasters can get done.</p>
	<hr/><br/>

	<h2>EmergencyPetMatcher</h2>
	<p>EmergencyPetMatcher (EPM) is a crowd-based web application that allows pet lovers and advocates at large to report, match, and verify lost and found pets as a collaborative effort during and after disaster events. EPM allows end-users to follow other users and coordinate in matching pets with their owners. The pet matches that are proposed in the system can be voted "up" or "down", and when a threshold is triggered, EPM will send messages to both parties encouraging human intervention to determine the success of the match. The application is not yet deployed but will be soon and can be accessed <a href="http://www.emergencypetmatcher.com">here</a>.</p>

	<img style="width:95%; display:block; margin:0 auto;" src="/assets/images/epm.png"/>
  <p style="text-align:center; font-weight:bold;">The EPM Home Page.</p>

  <p>EPM was a research effort for almost two years involving designers, developers, and researchers in studying online behaviors of pet reunion during mass displacement events. My colleague Joanne White recently published her work at CSCW on her ethnographic work examining an online pet community of pet advocates who self-organized to reunite pets with their owners on a Facebook page following the 2012 Hurricane Sandy event. She discovered that these digital volunteers mobilized to create an improvised case management system of photo albums and conversation threads to perform cross-validation, reporting, and matching work, the results of which were instrumental in reuniting many pets with their owners.</p>

  <p>EPM largely is not meant to be compared along other social media like Facebook pages. Rather, we are interested in how it can further support online pet matching behaviors that we have observed from White's work. Two major research goals are behind EPM: first, to use human computation as an engineering technique to solve an important societal problem and how crowdwork patterns can be leveraged to support motivations for engaging in this problem. Second, to examine the role that EPM will serve in the greater technological landscape of software solutions in response to disaster events. Naturally, as adoption grows, we would like to understand the features that EPM provides (or does not provide) that encourages its use. Software engineering issues, including issues of scalability, reliability, and usability, are first-class concerns to sustain its use in the wild. For more information, please check out the "Engineering for Crowdwork" paper.</p>

  <h2> EPIC Analyze </h2>
  <p>In crisis informatics research, efforts to understand the disaster-abetted social and online behaviors of the mass public is difficult. Data does not prove its value by content but by volume; thousands to millions of information pieces are shared across social media, including calls for help, volunteer opportunities, weather information, evacuation route notices, road closure warnings, and so on. These data are also ephemeral; software engineering efforts to collect, store, and then provision them for analysis face various challenges, such as responsiveness for interactive analytics, robustness to faults and crashes, and scale. Building and using analytical tools to handle the large information load is a research challenge, but then to support unique workflows of analysis that are unique to analysts who study these data further increases complexity of these tools.</p>

  <p>The Project EPIC lab has been working on these challenges since 2009. The first system built - called EPIC Collect and built by my colleague Aaron Schram - was targeted for collection of millions of tweets over hundreds of disaster events. Over its continued and heavy use, more recent work by Schram and Anderson have shown that migration to horizontally-scaling technologies such as NoSQL have improved the collection infrastructure by maintaining a 99.999% uptime since 2012. You can read their work from the Project EPIC Website <a href="http://epic.cs.colorado.edu">here</a>.</p>

  <img style="width:95%; display:block; margin:0 auto;" src="/assets/images/epic-analyze.png"/>
  <p style="text-align:center; font-weight:bold;">Analyzing Tweets from the 2013 Japan Earthquake Dataset.</p>

  <p>Collection is not enough to solve analytical challenges for the scale of data needed for crisis informatics research. Since 2013, I have led a software engineering team comprised of graduate student developers and researchers in building an analytics infrastructure to respond to the research challenges in providing interactive use of data at this scale. EPIC Analyze provides an analytics platform to provision the browsing, filtering, and visualization capabilities for examining large-scale Twitter datasets. Preliminary results from our work can be found in the "Analysis of Social Media Data" paper.</p>

  <img style="width:95%; display:block; margin:0 auto;" src="/assets/images/epic-analyze-architecture.jpg"/>
  <p style="text-align:center; font-weight:bold;">EPIC Analyze Architecture.</p>


</section>